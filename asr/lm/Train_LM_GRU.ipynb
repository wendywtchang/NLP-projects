{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_LM_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsRLT56pUDkOdvfGLVYygg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendywtchang/NLP-projects/blob/master/asr/lm/Train_LM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpskOcmMNmvp",
        "outputId": "db7dc6de-e47a-44e2-c52f-b0073d6bb992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/LM\n",
        "%ll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYkVVLscNpQC",
        "outputId": "8ce27b5c-5b49-4281-f2a2-d8d463035231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/LM\n",
            "total 483\n",
            "-rw------- 1 root      0 Feb  6 12:01 100\n",
            "drwx------ 2 root   4096 Nov 29 11:06 \u001b[0m\u001b[01;34maux_scripts\u001b[0m/\n",
            "drwx------ 2 root   4096 Nov 29 11:06 \u001b[01;34mconfig\u001b[0m/\n",
            "drwx------ 2 root   4096 Nov 29 11:06 \u001b[01;34mdata\u001b[0m/\n",
            "drwx------ 2 root   4096 Feb  4 23:16 \u001b[01;34mEsperBERTo\u001b[0m/\n",
            "drwx------ 2 root   4096 Nov 29 16:06 \u001b[01;34mgpt2\u001b[0m/\n",
            "-rw------- 1 root   7481 Nov 30 14:01 kneser_ney.py\n",
            "-rw------- 1 root   1082 Nov 29 11:05 LICENCE.md\n",
            "drwx------ 2 root   4096 Nov 29 11:06 \u001b[01;34mlogs\u001b[0m/\n",
            "drwx------ 2 root   4096 Nov 30 12:56 \u001b[01;34mmodels\u001b[0m/\n",
            "-rw------- 1 root 158976 Nov 29 11:05 paper_LREC18.pdf\n",
            "-rw------- 1 root   9035 Feb  5 23:16 perplexity.ipynb\n",
            "-rw------- 1 root  63073 Nov 29 11:05 poster_LREC18.pdf\n",
            "drwx------ 2 root   4096 Nov 30 14:01 \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw------- 1 root   8148 Nov 29 11:05 README.md\n",
            "drwx------ 2 root   4096 Nov 29 11:06 \u001b[01;34mscripts\u001b[0m/\n",
            "-rw------- 1 root  35632 Feb  6 13:29 Train_LM_GRU.ipynb\n",
            "-rw------- 1 root  37893 Feb  4 22:57 Train_lm_huggingface.ipynb\n",
            "-rw------- 1 root  87193 Feb  6 00:25 Train_LM_not_finished.ipynb\n",
            "-rw------- 1 root  20879 Feb  6 13:04 train_transformer.ipynb\n",
            "-rw------- 1 root  25362 Feb  6 10:24 transformer_tutorial.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sA4MhXGNU41",
        "outputId": "de47ede4-5c03-4b10-c8d6-831c3da67db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.25)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.71+cuda111)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->trax) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.62.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->trax) (3.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.54.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.23.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.13.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (13.0.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.43.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jf3ySrVMz9i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "import pickle\n",
        "import numpy\n",
        "import random as rnd\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "\n",
        "# import w2_unittest\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Dataset"
      ],
      "metadata": {
        "id": "Qc4A3odROFQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import codecs\n",
        "# f = codecs.open('./data/train/hound-train.txt', 'r', 'UTF-8')\n",
        "# for line in f:\n",
        "#     print(line)"
      ],
      "metadata": {
        "id": "oOrKon0mZnpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = 'data/'\n",
        "filename = 'train/hound-train.txt'\n",
        "lines = [] # storing all the lines in a variable. \n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:        \n",
        "        # remove leading and trailing whitespace\n",
        "        pure_line = line.strip()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)"
      ],
      "metadata": {
        "id": "XuHqnLEdM6Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6TbVVjwQnBO",
        "outputId": "72daa4e0-b5f3-4686-a771-b210015d56ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\",\n",
              " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
              " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
              " 're-use it under the terms of the Project Gutenberg License included',\n",
              " 'with this eBook or online at www.gutenberg.net',\n",
              " 'Title: The Adventures of Sherlock Holmes',\n",
              " 'Author: Arthur Conan Doyle',\n",
              " 'Release Date: November 29, 2002 [EBook #1661]',\n",
              " 'Last Updated: May 20, 2019',\n",
              " 'Language: English']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjccJESHQkHM",
        "outputId": "85ed20aa-07fd-4bbd-8be1-9c9e2960ec2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 9633\n",
            "Sample line at position 0 Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
            "Sample line at position 999 so that the trustees are at their wits’ end what to do with the money.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the letters are both uppercase and lowercase.  In order to reduce the complexity of the task, we will convert all characters to lowercase.  This way, the model only needs to predict the likelihood that a letter is 'a' and not decide between uppercase 'A' and lowercase 'a'."
      ],
      "metadata": {
        "id": "0neSCR7uQzjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# go through each line\n",
        "for i, line in enumerate(lines):\n",
        "    # convert to all lowercase\n",
        "    lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb7uQ4dMQrwh",
        "outputId": "f99d664f-3f81-4e6d-e9ea-68d5fbf01f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 9633\n",
            "Sample line at position 0 project gutenberg's the adventures of sherlock holmes, by arthur conan doyle\n",
            "Sample line at position 999 so that the trustees are at their wits’ end what to do with the money.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSfxW_nMQ2z-",
        "outputId": "cf83ca6b-a083-4618-9e63-8d4ee9d31d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines for training: 8633\n",
            "Number of lines for validation: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Covert a line to tensor\n",
        "\n",
        "Now that you have your list of lines, you will convert each character in that list to a number. You can use Python's `ord` function to do it. \n",
        "\n",
        "Given a string representing of one Unicode character, the `ord` function return an integer representing the Unicode code point of that character."
      ],
      "metadata": {
        "id": "FeBMZE47Q_yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the unique unicode integer associated with each character\n",
        "print(f\"ord('a'): {ord('a')}\")\n",
        "print(f\"ord('b'): {ord('b')}\")\n",
        "print(f\"ord('c'): {ord('c')}\")\n",
        "print(f\"ord(' '): {ord(' ')}\")\n",
        "print(f\"ord('x'): {ord('x')}\")\n",
        "print(f\"ord('y'): {ord('y')}\")\n",
        "print(f\"ord('z'): {ord('z')}\")\n",
        "print(f\"ord('1'): {ord('1')}\")\n",
        "print(f\"ord('2'): {ord('2')}\")\n",
        "print(f\"ord('3'): {ord('3')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le94cAE9Q6Z9",
        "outputId": "b2acaa1a-2c81-4a37-c311-f5bb35d21395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ord('a'): 97\n",
            "ord('b'): 98\n",
            "ord('c'): 99\n",
            "ord(' '): 32\n",
            "ord('x'): 120\n",
            "ord('y'): 121\n",
            "ord('z'): 122\n",
            "ord('1'): 49\n",
            "ord('2'): 50\n",
            "ord('3'): 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def line_to_tensor(line, EOS_int=1):\n",
        "    \"\"\"Turns a line of text into a tensor\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        list: a list of integers (unicode values) for the characters in the `line`.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the tensor as an empty list\n",
        "    tensor = []\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # for each character:\n",
        "    for c in line:\n",
        "        \n",
        "        # convert to unicode int\n",
        "        c_int = ord(c)\n",
        "        \n",
        "        # append the unicode integer to the tensor list\n",
        "        tensor.append(c_int)\n",
        "    \n",
        "    # include the end-of-sentence integer\n",
        "    tensor.append(EOS_int)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "BirpykCBRTZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing your output\n",
        "line_to_tensor('abc xyz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BXGm29qRw7G",
        "outputId": "518c8854-63a2-456e-f0f3-1f9cc387ff43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 98, 99, 32, 120, 121, 122, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "                Keep in mind that the length of the tensor is always 1 + the length\n",
        "                of the original line of characters.\n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "\n",
        "    Yields:\n",
        "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
        "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater than or equal to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index) \n",
        "                            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index += 1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                # so that the tensor plus padding will have length `max_length`\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for this tensor_pad is 1 whereever tensor_pad is not\n",
        "                # 0 and 0 whereever tensor_pad is 0, i.e. if tensor_pad is\n",
        "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
        "                # [1, 1, 1, 0, 0, 0]\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask) # @ KEEPTHIS\n",
        "               \n",
        "            # convert the batch (data type list) to a numpy array\n",
        "            batch_np_arr = np.array(batch)\n",
        "            mask_np_arr = np.array(mask)\n",
        "            \n",
        "            ### END CODE HERE ##\n",
        "            \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []\n",
        "            "
      ],
      "metadata": {
        "id": "MsI_78-bR1UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out your data generator\n",
        "tmp_lines = ['12345678901', #length 11\n",
        "             '123456789', # length 9\n",
        "             '234567890', # length 9\n",
        "             '345678901'] # length 9\n",
        "\n",
        "# Get a batch size of 2, max length 10\n",
        "tmp_data_gen = data_generator(batch_size=2, \n",
        "                              max_length=10, \n",
        "                              data_lines=tmp_lines,\n",
        "                              shuffle=False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_data_gen)\n",
        "\n",
        "# view the batch\n",
        "tmp_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiRRjwBuTD4T",
        "outputId": "1c9fd8f8-f982-4721-f59d-e6ee73796b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Repeating Batch generator\n",
        "\n",
        "The way the iterator is currently defined, it will keep providing batches forever.\n",
        "\n",
        "Although it is not needed, we want to show you the `itertools.cycle` function which is really useful when the generator eventually stops\n",
        "\n",
        "Notice that it is expected to use this function within the training function further below\n",
        "\n",
        "Usually we want to cycle over the dataset multiple times during training (i.e. train for multiple *epochs*).\n",
        "\n",
        "For small datasets we can use [`itertools.cycle`](https://docs.python.org/3.8/library/itertools.html#itertools.cycle) to achieve this easily."
      ],
      "metadata": {
        "id": "xgmrlObZTOBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "infinite_data_generator = itertools.cycle(\n",
        "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
      ],
      "metadata": {
        "id": "76fvY9jyTHp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
        "print(len(ten_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmCNVPqLTYW-",
        "outputId": "c3599395-9c8a-4f9a-9ecb-7d247cdc898b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the GRU model\n",
        "\n",
        "Now that you have the input and output tensors, you will go ahead and initialize your model. You will be implementing the `GRULM`, gated recurrent unit model. To implement this model, you will be using google's `trax` package. Instead of making you implement the `GRU` from scratch, we will give you the necessary methods from a build in package. You can use the following packages when constructing the model: \n",
        "\n",
        "\n",
        "- `tl.Serial`: Combinator that applies layers serially (by function composition). [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/combinators.py#L26)\n",
        "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
        "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`\n",
        "\n",
        "___\n",
        "\n",
        "- `tl.ShiftRight`: Allows the model to go right in the feed forward. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/attention.py#L560)\n",
        "    - `ShiftRight(n_shifts=1, mode='train')` layer to shift the tensor to the right n_shift times\n",
        "    - Here in the exercise you only need to specify the mode and not worry about n_shifts\n",
        "\n",
        "___\n",
        "\n",
        "- `tl.Embedding`: Initializes the embedding. In this case it is the size of the vocabulary by the dimension of the model. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L130) \n",
        "    - `tl.Embedding(vocab_size, d_feature)`.\n",
        "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
        "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
        "___\n",
        "\n",
        "- `tl.GRU`: `Trax` GRU layer. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRU) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/rnn.py#L154)\n",
        "    - `GRU(n_units)` Builds a traditional GRU of n_cells with dense internal transformations.\n",
        "    - `GRU` paper: https://arxiv.org/abs/1412.3555\n",
        "___\n",
        "\n",
        "- `tl.Dense`: A dense layer. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L34)\n",
        "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.\n",
        "___\n",
        "\n",
        "- `tl.LogSoftmax`: Log of the output probabilities. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L644)\n",
        "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
        "___\n",
        "\n",
        "<a name='ex03'></a>\n",
        "### Exercise 03\n",
        "**Instructions:** Implement the `GRULM` class below. You should be using all the methods explained above.\n"
      ],
      "metadata": {
        "id": "rLc7H3_BTnqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: GRULM\n",
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    \"\"\"Returns a GRU language model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
        "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
        "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    model = tl.Serial(\n",
        "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax() # Log Softmax\n",
        "    ) \n",
        "    ### END CODE HERE ###\n",
        "    return model"
      ],
      "metadata": {
        "id": "AvygPxG7Taks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing your model\n",
        "model = GRULM()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReymNskeUFVY",
        "outputId": "d90c32d2-3136-4858-9543-681993ae16a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_256_512\n",
            "  GRU_512\n",
            "  GRU_512\n",
            "  Dense_256\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "cGkF2gzNUSz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ],
      "metadata": {
        "id": "YtFD7COMUHoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_used_lines(lines, max_length):\n",
        "    '''\n",
        "    Args: \n",
        "    lines: all lines of text an array of lines\n",
        "    max_length - max_length of a line in order to be considered an int\n",
        "    output_dir - folder to save your file an int\n",
        "    Return:\n",
        "    number of efective examples\n",
        "    '''\n",
        "\n",
        "    n_lines = 0\n",
        "    for l in lines:\n",
        "        if len(l) <= max_length:\n",
        "            n_lines += 1\n",
        "    return n_lines\n",
        "\n",
        "num_used_lines = n_used_lines(lines, 32)\n",
        "print('Number of used lines from the dataset:', num_used_lines)\n",
        "print('Batch size (a power of 2):', int(batch_size))\n",
        "steps_per_epoch = int(num_used_lines/batch_size)\n",
        "print('Number of steps to cover one epoch:', steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-QGiJKPUYyq",
        "outputId": "fee2482b-5921-444e-fa3a-ef11561a0b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of used lines from the dataset: 1179\n",
            "Batch size (a power of 2): 32\n",
            "Number of steps to cover one epoch: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: train_model\n",
        "def train_model(model, data_generator, lines, eval_lines, batch_size=32, max_length=64, n_steps=1, output_dir='model/'): \n",
        "    \"\"\"Function that trains the model\n",
        "\n",
        "    Args:\n",
        "        model (trax.layers.combinators.Serial): GRU model.\n",
        "        data_generator (function): Data generator function.\n",
        "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
        "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
        "        lines (list): List of lines to use for training. Defaults to lines.\n",
        "        eval_lines (list): List of lines to use for evaluation. Defaults to eval_lines.\n",
        "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
        "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
        "\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
        "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "    \n",
        "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
        "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "    \n",
        "    train_task = training.TrainTask( \n",
        "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
        "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
        "        optimizer=trax.optimizers.Adam(0.0005)     # Don't forget to add the learning rate parameter TO 0.0005\n",
        "    ) \n",
        "    \n",
        "    eval_task = training.EvalTask( \n",
        "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
        "        n_eval_batches=3  # For better evaluation accuracy in reasonable time \n",
        "    ) \n",
        "    \n",
        "    training_loop = training.Loop(model, \n",
        "                                  train_task, \n",
        "                                  eval_tasks=[eval_task], \n",
        "                                  output_dir=output_dir) \n",
        "\n",
        "    training_loop.run(n_steps=n_steps)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # We return this because it contains a handle to the model, which has the weights etc.\n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "1nKeStp-UcV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
        "output_dir = './GRU-model/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(output_dir)\n",
        "except OSError as e:\n",
        "    pass\n",
        "\n",
        "training_loop = train_model(GRULM(), data_generator, lines=lines, eval_lines=eval_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhzlFLCtVDVZ",
        "outputId": "4ce511b3-5520-46aa-e0bb-c29371bea2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:413: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 3411200\n",
            "Step      1: Ran 1 train steps in 7.51 secs\n",
            "Step      1: train CrossEntropyLoss |  5.38078070\n",
            "Step      1: eval  CrossEntropyLoss |  5.45253865\n",
            "Step      1: eval          Accuracy |  0.14538801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "- Evaluating using the deep nets"
      ],
      "metadata": {
        "id": "VLQG6pOaVYST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: test_model\n",
        "def test_model(preds, target):\n",
        "    \"\"\"Function to test the model.\n",
        "\n",
        "    Args:\n",
        "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
        "\n",
        "    Returns:\n",
        "        float: log_perplexity of the model.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # log_p = np.sum(None * None, axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
        "\n",
        "    # non_pad = 1.0 - np.equal(None, None)          # You should check if the target equals 0\n",
        "    # log_p = None * None                             # Get rid of the padding    \n",
        "    \n",
        "    # log_ppx = np.sum(None, None) / np.sum(None, None) # Remember to set the axis properly when summing up\n",
        "    # log_ppx = np.mean(None) # Compute the mean of the previous expression\n",
        "\n",
        "    ###############################\n",
        "\n",
        "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
        "\n",
        "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
        "    ppx = total_log_ppx * non_pad                       # Get rid of the padding\n",
        "\n",
        "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
        "    \n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return -log_ppx"
      ],
      "metadata": {
        "id": "8hnJXkrbVGcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# Testing \n",
        "model = GRULM()\n",
        "model.init_from_file('./GRU-model/model.pkl.gz')\n",
        "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
        "preds = model(batch[0])\n",
        "log_ppx, ppx = test_model(preds, batch[1])\n",
        "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9m265TeXNaB",
        "outputId": "6ee4630f-ae2f-4c2f-eb2b-c44fe606d454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log perplexity and perplexity of your model are respectively 5.5314364 252.50635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fVB_Upx6XSLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to generate some news sentence\n",
        "def gumbel_sample(log_probs, temperature=1.0):\n",
        "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
        "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "    g = -np.log(-np.log(u))\n",
        "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
        "\n",
        "def predict(num_chars, prefix):\n",
        "    inp = [ord(c) for c in prefix]\n",
        "    result = [c for c in prefix]\n",
        "    max_len = len(prefix) + num_chars\n",
        "    for _ in range(num_chars):\n",
        "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
        "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
        "        next_char = gumbel_sample(outp[0, len(inp)])\n",
        "        inp += [int(next_char)]\n",
        "       \n",
        "        if inp[-1] == 1:\n",
        "            break  # EOS\n",
        "        result.append(chr(int(next_char)))\n",
        "    \n",
        "    return \"\".join(result)\n",
        "\n",
        "print(predict(32, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2-xkH6UYxAi",
        "outputId": "149e4387-8acf-46b1-8fa3-489573ed4f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ïtéKQ]qÝzvøô=\u001cÙ;àQ\u000bklÈOÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuHUQe95Yx5U",
        "outputId": "6a4bd390-97ec-4393-f670-fb228fbab770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï¨ò£\u0000;\u0018¢´â½íap5C\fâÁ{  \u0010ñ&³Å¤m\u0016\n",
            "(\u001d-cð±Á?¿Lå\u0002\u001f\u0015\u0005\u0003UÀ\u0018äêÂ\u0006Ë\u001c\n",
            "ÇÀ»kòÂ¤pxÃÃ/g±÷5N\u000e\u0002û²'6ø{\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ucFrG66CY9kJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}